Sophie Gunn

Notes on Chapter 4: Classification

Problem: What do we do when our response variable is not quantitative?
We want to predict a qualitative response for an observation, aka classify that response. 

can use a dummy variable when working with a binary qualitative response

logistic regression:

logistic regression models the probability that Y belonds to a particular category. 

we can use a linear regression model to represent the probabilities

p(X) = beta_0 + beta_1*X

but we need our values to be (0,1), so


p(X) = e^(beta_0 + beta_1*X)/ 1 + e^(beta_0 + beta_1*X)
to fit this model we use maximum likelihood

can write 
p(X)/ (1 - p(X)) = e^(beta_0 + beta_1*X)
which is the function for the odds, we also like log odds:

log(p(X)/ (1 - p(X))) = beta_0 + beta_1*X

the amount that p(X) changes due to a one-unit change in X will depend on the current value of X, but regardless of the value of X, if beta_1 is positive then increasing X will be associated with increasing p(X). 


we try to find beta_0 and beta_1 such that plugging these estimates into the model for p(X) yields a high number close to one for all individuals who defaulted, and a number close to zero for all individuals who did not. 


~we can have multiple classes, but not used that often, discriminant linear  analysis is more popular~

linear discriminant analysis:

with LDA we model the distribution of the predictors X seperately in each of the response classes, and then use Bayes' theorem to flip these around into estimates for P(Y = k|X = x)
	LDA tends to be more stable than logistic

P(Y = k|X = x) ~ pi_k * f_k(x)
pi_k = prob that a given observation comes from the kth class (prior)
f_k(x) = the density function of X for an observation that comes from the kth class. (likelihood)

LDA classifer results from assuming that the observations within each class come from a normal distribution with a class-specific mean vector and a comman variance sigma^2, and estiates for these parameters into the bayes classifer.

*can also allow observations in kth class to have class-specific variance sigma_k^2.

we can tune our specificity (type 1 error) and sensitivity/power (type 2 error) by changing the threshold for classification.

comparison of classification methods:

LDA assumes normal distribution, logistic uses maximum likelihood.
thus, when normality assumption holds, LDA performs better.

alternatively, KNN is completely nonparametric.

when decision boundaries are linear, LDA and logistic work best.
when not, QDA may be better.
for much more complicated decision boundaries, KNN may be better

*but must choose appropriate level of smoothness*

