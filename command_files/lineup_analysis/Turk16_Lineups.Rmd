---
title: "Turk16_Lineups"
author: "Aidan Mullan"
date: "3/28/2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(nullabor)
library(ggplot2)
library(dplyr)
library(scagnostics)
library(MASS)
library(tidyverse)
library(caret)
library(randomForest)

scagnostics <- read.csv("simulation_data/turk16_scagnostics.csv")
scagnostics <- scagnostics[,-1]
info <- read.csv("simulation_data/turk16_info.csv")
info <- info[,-1]
plots <- read.csv("simulation_data/turk16_plots.csv")
plots <- plots[,-1]

scagnostics$set <- info$set
```

```{r}
choice <- 16
choice_plots <- subset(plots, plots$set == choice)
choice_info <- subset(info, info$set == choice)
cat(subset(choice_info, choice_info$signal == 1)$in.lineup.ID)

#To scale all plots, scales = "free" in facet_wrap
ggplot(choice_plots, aes(x, y)) +
  geom_point() +
  facet_wrap(~in.lineup.ID, nrow = 4, labeller = label_context) 
```



```{r, warning = FALSE}
###NEEDS WORK, TEST AUTHENTICITY OF PREDICTIONS
#Clumpy scagnostic doesnt seem to be detecting cluster signal plots

choices <- data.frame(correct.choice = numeric(20), eu.choice = numeric(20), maha.choice = numeric(20), rf.choice = numeric(20), knn.choice = numeric(20), lda.choice = numeric(20), logit.choice = numeric(20))
for(i in 1:20){
  print(i)
  test_data <- subset(scagnostics, scagnostics$set == i)
  train_data <- subset(scagnostics, scagnostics$set != i)
  choices$correct.choice[i] <- which(test_data$signal == 1)

  model_LDA <- lda(signal~scag_num_1+scag_num_2+scag_num_3+scag_num_4+scag_num_5+
                     scag_num_6+scag_num_6+scag_num_7+scag_num_8+scag_num_9, data = train_data)
  LDA_preds <- predict(model_LDA, test_data, type = "response")
  choices$lda.choice[i] <- which(LDA_preds$posterior[,2] == max(LDA_preds$posterior[,2]))
  
  model_logit <- glm(signal~scag_num_1+scag_num_2+scag_num_3+scag_num_4+scag_num_5+
                       scag_num_6+scag_num_7+scag_num_8+scag_num_9, 
                     data = train_data, family ="binomial")
  logit_preds <- predict(model_logit, test_data, type = "response")
  choices$logit.choice[i] <- which(logit_preds == max(logit_preds))
  
  control <- trainControl(method = "cv", number = 10, classProbs = TRUE, returnData = TRUE)
  model_knn <- train(make.names(as.factor(signal))~scag_num_1+scag_num_2+scag_num_3+scag_num_4+
                       scag_num_5+scag_num_6+scag_num_6+scag_num_7+scag_num_8+scag_num_9, 
                     data = train_data, method = "knn", trControl = control, 
                   tuneGrid = expand.grid(k = 1:25))
  knn_preds <- predict.train(model_knn, test_data, type = "prob")
  choices$knn.choice[i] <- which(knn_preds[,2] == max(knn_preds[,2]))
  
  model_rf <- randomForest(as.factor(signal) ~scag_num_1+scag_num_2+scag_num_3+scag_num_4+
                             scag_num_5+scag_num_6+scag_num_7+scag_num_8+scag_num_9, data = train_data,
                           ntree=100, importance =T)
  rf_preds <- predict(model_rf, newdata = test_data[,2:10], type = "prob")
  choices$rf.choice[i] <- which(rf_preds[,2] == max(rf_preds[,2]))  
  
  dscags <- test_data[,2:10]
  means <- colMeans(dscags)
  eu_dists <- NULL
  for (j in 1:20){
    eu_dists <- c(eu_dists, (dist(rbind(dscags[j,],means))))
  }
  choices$eu.choice[i] <- which(eu_dists == max(eu_dists))
  
  mah_dists <- mahalanobis(dscags, means, cov(dscags))
  choices$maha.choice[i] <- which(mah_dists == max(mah_dists))
}

accuracy <- data.frame(Euclidean = mean(choices$eu.choice == choices$correct.choice), 
                       Mahalanobis = mean(choices$maha.choice == choices$correct.choice),
                       LDA = mean(choices$lda.choice == choices$correct.choice),
                       Logistic = mean(choices$logit.choice == choices$correct.choice),
                       KNN = mean(choices$knn.choice == choices$correct.choice),
                       Random.Forest = mean(choices$rf.choice == choices$correct.choice))
accuracy
```


```{r, warning = FALSE}
###Look into why accuracy is so low

old_scags_raw <- read.csv("simulation_data/Cluster_Scagnostics.csv")
old_scags <- old_scags_raw[,-1]
old_info_raw <- read.csv("simulation_data/Cluster_Info.csv")
old_info <- old_info_raw[,-1]

old_scags <- old_scags %>% spread(key = scag_num, value = scagnostics, sep = "_")

model_LDA <- lda(signal~scag_num_1+scag_num_2+scag_num_3+scag_num_4+scag_num_5+
                     scag_num_6+scag_num_6+scag_num_7+scag_num_9, data = old_scags)
model_logit <- glm(signal~scag_num_1+scag_num_2+scag_num_3+scag_num_4+scag_num_5+
                       scag_num_6+scag_num_7+scag_num_9, 
                     data = old_scags, family ="binomial")
control <- trainControl(method = "cv", number = 10, classProbs = TRUE, returnData = TRUE)
model_knn <- train(make.names(as.factor(signal))~scag_num_1+scag_num_2+scag_num_3+scag_num_4+
                       scag_num_5+scag_num_6+scag_num_6+scag_num_7+scag_num_9, 
                     data = old_scags, method = "knn", trControl = control, 
                   tuneGrid = expand.grid(k = 1:25))
model_rf <- randomForest(as.factor(signal) ~scag_num_1+scag_num_2+scag_num_3+scag_num_4+
                             scag_num_5+scag_num_6+scag_num_7+scag_num_9, data = old_scags,
                           ntree=100, importance =T)

choices <- data.frame(correct.choice = numeric(20), eu.choice = numeric(20), maha.choice = numeric(20), rf.choice = numeric(20), knn.choice = numeric(20), lda.choice = numeric(20), logit.choice = numeric(20))
for(i in 1:20){
  print(i)
  test_data <- subset(scagnostics, scagnostics$set == i)
  choices$correct.choice[i] <- which(test_data$signal == 1)


  LDA_preds <- predict(model_LDA, test_data, type = "response")
  choices$lda.choice[i] <- which(LDA_preds$posterior[,2] == max(LDA_preds$posterior[,2]))
  
  logit_preds <- predict(model_logit, test_data, type = "response")
  choices$logit.choice[i] <- which(logit_preds == max(logit_preds))

  knn_preds <- predict.train(model_knn, test_data, type = "prob")
  choices$knn.choice[i] <- which(knn_preds[,2] == max(knn_preds[,2]))
  
  rf_preds <- predict(model_rf, newdata = test_data[,2:10], type = "prob")
  choices$rf.choice[i] <- which(rf_preds[,2] == max(rf_preds[,2]))  
  
  dscags <- test_data[,2:10]
  means <- colMeans(dscags)
  eu_dists <- NULL
  for (j in 1:20){
    eu_dists <- c(eu_dists, (dist(rbind(dscags[j,],means))))
  }
  choices$eu.choice[i] <- which(eu_dists == max(eu_dists))
  
  mah_dists <- mahalanobis(dscags, means, cov(dscags))
  choices$maha.choice[i] <- which(mah_dists == max(mah_dists))
}

accuracy <- data.frame(Euclidean = mean(choices$eu.choice == choices$correct.choice), 
                       Mahalanobis = mean(choices$maha.choice == choices$correct.choice),
                       LDA = mean(choices$lda.choice == choices$correct.choice),
                       Logistic = mean(choices$logit.choice == choices$correct.choice),
                       KNN = mean(choices$knn.choice == choices$correct.choice),
                       Random.Forest = mean(choices$rf.choice == choices$correct.choice))


accuracy
```

```{r}
scag <- data.frame("1"= numeric(2),"2"= numeric(2),"3"= numeric(2),"4"= numeric(2),"5"= numeric(2),"6"= numeric(2),"7"= numeric(2),"8"= numeric(2),"9"= numeric(2))
for(i in 1:20){
  plot <- subset(choice_plots, choice_plots$in.lineup.ID == i)
  scag[i,] <- scagnostics(plot$x, plot$y)$s
}

test <- data.frame(correct.choice = numeric(20), model.choice = numeric(20))
for(i in 1:20){
  print(i)
  test_data <- subset(scagnostics, scagnostics$set == i)
  train_data <- subset(scagnostics, scagnostics$set != i)
  test$correct.choice[i] <- which(test_data$signal == 1)
  
  scag2 <- test_data[,2:10]
  means <- colMeans(scag)
  eu_dists <- NULL
  for (j in 1:20){
    eu_dists <- c(eu_dists, (dist(rbind(scag[j,],means))))
  }
  test$model.choice[i] <- which(eu_dists == max(eu_dists))
}

means <- colMeans(scag)
eu_dists <- NULL
for (j in 1:20){
  eu_dists <- c(eu_dists, (dist(rbind(scag[j,],means))))
}
which(eu_dists == max(eu_dists))
```

```{r, warning = FALSE}
#Look into redoing scagnostics for this data

choices <- data.frame(correct.choice = numeric(20), eu.choice = numeric(20), maha.choice = numeric(20), rf.choice = numeric(20), knn.choice = numeric(20), lda.choice = numeric(20), logit.choice = numeric(20))
for(i in 1:20){
  print(i)
  choice_plots <- subset(plots, plots$set == i)
  choice_info <- subset(info, info$set == i)
  train_data <- subset(scagnostics, scagnostics$set != i)
  test_data <- data.frame("scag_num_1"= numeric(2),"scag_num_2"= numeric(2),"scag_num_3"= numeric(2),"scag_num_4"= numeric(2),"scag_num_5"= numeric(2),"scag_num_6"= numeric(2),"scag_num_7"= numeric(2),"scag_num_8"= numeric(2),"scag_num_9"= numeric(2))

  for(k in 1:20){
    plot <- subset(choice_plots, choice_plots$in.lineup.ID == k)
    test_data[k,] <- scagnostics(plot$x, plot$y)$s
  }
  choices$correct.choice[i] <- which(choice_info$signal == 1)
  
  means <- colMeans(test_data)
  eu_dists <- NULL
  for (j in 1:20){
    eu_dists <- c(eu_dists, (dist(rbind(test_data[j,],means))))
  }
  choices$eu.choice[i] <- which(eu_dists == max(eu_dists))
  
  mah_dists <- mahalanobis(test_data, means, cov(test_data))
  choices$maha.choice[i] <- which(mah_dists == max(mah_dists))
  
  
  model_LDA <- lda(signal~scag_num_1+scag_num_2+scag_num_3+scag_num_4+scag_num_5+
                     scag_num_6+scag_num_6+scag_num_7+scag_num_8+scag_num_9, data = train_data)
  LDA_preds <- predict(model_LDA, test_data, type = "response")
  choices$lda.choice[i] <- which(LDA_preds$posterior[,2] == max(LDA_preds$posterior[,2]))
  
  model_logit <- glm(signal~scag_num_1+scag_num_2+scag_num_3+scag_num_4+scag_num_5+
                       scag_num_6+scag_num_7+scag_num_8+scag_num_9, 
                     data = train_data, family ="binomial")
  logit_preds <- predict(model_logit, test_data, type = "response")
  choices$logit.choice[i] <- which(logit_preds == max(logit_preds))
  
  control <- trainControl(method = "cv", number = 10, classProbs = TRUE, returnData = TRUE)
  model_knn <- train(make.names(as.factor(signal))~scag_num_1+scag_num_2+scag_num_3+scag_num_4+
                       scag_num_5+scag_num_6+scag_num_6+scag_num_7+scag_num_8+scag_num_9, 
                     data = train_data, method = "knn", trControl = control, 
                   tuneGrid = expand.grid(k = 1:25))
  knn_preds <- predict.train(model_knn, test_data, type = "prob")
  choices$knn.choice[i] <- which(knn_preds[,2] == max(knn_preds[,2]))
  
  model_rf <- randomForest(as.factor(signal) ~scag_num_1+scag_num_2+scag_num_3+scag_num_4+
                             scag_num_5+scag_num_6+scag_num_7+scag_num_8+scag_num_9, data = train_data,
                           ntree=100, importance =T)
  rf_preds <- predict(model_rf, newdata = test_data, type = "prob")
  choices$rf.choice[i] <- which(rf_preds[,2] == max(rf_preds[,2])) 
}

accuracy <- data.frame(Euclidean = mean(choices$eu.choice == choices$correct.choice), 
                       Mahalanobis = mean(choices$maha.choice == choices$correct.choice),
                       LDA = mean(choices$lda.choice == choices$correct.choice),
                       Logistic = mean(choices$logit.choice == choices$correct.choice),
                       KNN = mean(choices$knn.choice == choices$correct.choice),
                       Random.Forest = mean(choices$rf.choice == choices$correct.choice))
accuracy
```


