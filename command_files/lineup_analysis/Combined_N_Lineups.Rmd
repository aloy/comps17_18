---
title: "Combined_N_Lineups"
author: "Aidan Mullan"
date: "4/3/2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(nullabor)
library(ggplot2)
library(dplyr)
library(scagnostics)
library(MASS)

scagnostics <- read.csv("simulation_data/combined_scagnostics.csv")
scagnostics <- scagnostics[,3:13]
names(scagnostics)[2:10] <- c("scag_num_1", "scag_num_2", "scag_num_3", "scag_num_4", "scag_num_5", "scag_num_6", "scag_num_7", "scag_num_8", "scag_num_9")

scagnostics$ID <- 1:29100
signal <- subset(scagnostics, scagnostics$signal == 1)
null <- subset(scagnostics, scagnostics$signal == 0)
```


```{r}
#Predictions with known n
K <- 1000
choices <- data.frame(correct.choice = numeric(K), LDA.choice = numeric(K), QDA.choice = numeric(K), LOG.choice = numeric(K), KNN.choice = numeric(K), RF.choice = numeric(K), EU.choice = numeric(K), MAH.choice = numeric(K))

for(i in 1:K){
  print(i)
  n <- sample(1:20, 1)

  lineup_signal <- sample_n(signal, n)
  lineup_null <- sample_n(null, 20-n)
  lineup_scags <- rbind(lineup_signal, lineup_null) 
  lineup_scags$position <- sample(1:20, 20)
  test_data <- lineup_scags %>% arrange(position)
  choices$correct.choice[i] <- list(which(test_data$signal == 1))
  
  index <- which(scagnostics$ID %in% lineup_scags$ID)
  other_data <- scagnostics[-index,]
  train_data <- sample_n(other_data, 3000)
  
  model_LDA <- lda(signal~scag_num_1+scag_num_2+scag_num_3+scag_num_4+scag_num_5+
                     scag_num_6+scag_num_6+scag_num_7+scag_num_8+scag_num_9, data = train_data)
  LDA_preds <- predict(model_LDA, test_data, type = "response")
  LDA_max <- sort(LDA_preds$posterior[,2], decreasing = TRUE)[1:n]
  choices$LDA.choice[i] <- list(which(LDA_preds$posterior[,2] %in% LDA_max))
  
  model_QDA <- qda(signal~scag_num_1+scag_num_2+scag_num_3+scag_num_4+scag_num_5+
                     scag_num_6+scag_num_6+scag_num_7+scag_num_8+scag_num_9, data = train_data)
  QDA_preds <- predict(model_QDA, test_data, type = "response")
  QDA_max <- sort(QDA_preds$posterior[,2], decreasing = TRUE)[1:n]
  choices$QDA.choice[i] <- list(which(QDA_preds$posterior[,2] %in% QDA_max))
  
  model_logit <- glm(signal~scag_num_1+scag_num_2+scag_num_3+scag_num_4+scag_num_5+
                       scag_num_6+scag_num_7+scag_num_8+scag_num_9, 
                     data = train_data, family ="binomial")
  logit_preds <- predict(model_logit, test_data, type = "response")
  logit_max <- sort(logit_preds, decreasing = TRUE)[1:n]
  choices$LOG.choice[i] <- list(which(logit_preds %in% logit_max))
  
  control <- trainControl(method = "cv", number = 10, classProbs = TRUE, returnData = TRUE)
  model_knn <- train(make.names(as.factor(signal))~scag_num_1+scag_num_2+scag_num_3+scag_num_4+
                       scag_num_5+scag_num_6+scag_num_6+scag_num_7+scag_num_8+scag_num_9, 
                     data = train_data, method = "knn", trControl = control, 
                   tuneGrid = expand.grid(k = 1:25))
  knn_preds <- predict.train(model_knn, test_data, type = "prob")
  knn_max <- sort(knn_preds[,2], decreasing = TRUE)[1:n]
  choices$KNN.choice[i] <- list(which(knn_preds[,2] %in% knn_max))
  
  model_rf <- randomForest(as.factor(signal) ~scag_num_1+scag_num_2+scag_num_3+scag_num_4+
                             scag_num_5+scag_num_6+scag_num_7+scag_num_8+scag_num_9, data = train_data,
                           ntree=100, importance =T)
  rf_preds <- predict(model_rf, newdata = test_data[,2:10], type = "prob")
  rf_max <- sort(rf_preds[,2], decreasing = TRUE)[1:n]
  choices$RF.choice[i] <- list(which(rf_preds[,2] %in% rf_max)) 
  
  dscags <- test_data[,2:10]
  means <- colMeans(dscags)
  eu_dists <- NULL
  for (j in 1:20){
    eu_dists <- c(eu_dists, (dist(rbind(dscags[j,],means))))
  }
  eu_max <- sort(eu_dists, decreasing = TRUE)[1:n]
  choices$EU.choice[i] <- list(which(eu_dists %in% eu_max))
  
  mah_dists <- mahalanobis(dscags, means, cov(dscags))
  mah_max <- sort(mah_dists, decreasing = TRUE)[1:n]
  choices$MAH.choice[i] <- list(which(mah_dists %in% mah_max))
}

accuracy <- data.frame(Euclidean = numeric(K), Mahalanobis = numeric(K), LDA = numeric(K), QDA = numeric(K), Logistic = numeric(K), KNN = numeric(K), Random.Forest = numeric(K))

for(set in 1:K){
  correct <- choices$correct.choice[[set]]
  accuracy$Euclidean[set] <- mean(choices$EU.choice[[set]] %in% correct)
  accuracy$Mahalanobis[set] <- mean(choices$MAH.choice[[set]] %in% correct)
  accuracy$LDA[set] <- mean(choices$LDA.choice[[set]] %in% correct)
  accuracy$QDA[set] <- mean(choices$QDA.choice[[set]] %in% correct)
  accuracy$Logistic[set] <- mean(choices$LOG.choice[[set]] %in% correct)
  accuracy$KNN[set] <- mean(choices$KNN.choice[[set]] %in% correct)
  accuracy$Random.Forest[set] <- mean(choices$RF.choice[[set]] %in% correct)
}   

table1 <- colMeans(accuracy)
#EU - .826, MAH - .703, LDA - .955, QDA - .985, LOG - .983, KNN - .941, RF - .989
```

```{r}
#Predictions with unknown n
K <- 1000
choices <- data.frame(correct.choice = numeric(K), LDA.choice = numeric(K), QDA.choice = numeric(K), LOG.choice = numeric(K), KNN.choice = numeric(K), RF.choice = numeric(K), EU.choice = numeric(K), MAH.choice = numeric(K))

for(i in 1:K){
  print(i)
  n <- sample(1:20, 1)

  lineup_signal <- sample_n(signal, n)
  lineup_null <- sample_n(null, 20-n)
  lineup_scags <- rbind(lineup_signal, lineup_null) 
  lineup_scags$position <- sample(1:20, 20)
  test_data <- lineup_scags %>% arrange(position)
  choices$correct.choice[i] <- list(which(test_data$signal == 1))
  
  index <- which(scagnostics$ID %in% lineup_scags$ID)
  other_data <- scagnostics[-index,]
  train_data <- sample_n(other_data, 3000)
  
  model_LDA <- lda(signal~scag_num_1+scag_num_2+scag_num_3+scag_num_4+scag_num_5+
                     scag_num_6+scag_num_6+scag_num_7+scag_num_8+scag_num_9, data = train_data)
  LDA_preds <- predict(model_LDA, test_data, type = "response")
  choices$LDA.choice[i] <- list(which(LDA_preds$posterior[,2] > 0.5))
  
  model_QDA <- qda(signal~scag_num_1+scag_num_2+scag_num_3+scag_num_4+scag_num_5+
                     scag_num_6+scag_num_6+scag_num_7+scag_num_8+scag_num_9, data = train_data)
  QDA_preds <- predict(model_QDA, test_data, type = "response")
  choices$QDA.choice[i] <- list(which(QDA_preds$posterior[,2] > 0.5))
  
  model_logit <- glm(signal~scag_num_1+scag_num_2+scag_num_3+scag_num_4+scag_num_5+
                       scag_num_6+scag_num_7+scag_num_8+scag_num_9, 
                     data = train_data, family ="binomial")
  logit_preds <- predict(model_logit, test_data, type = "response")
  choices$LOG.choice[i] <- list(which(logit_preds > 0.5))
  
  control <- trainControl(method = "cv", number = 10, classProbs = TRUE, returnData = TRUE)
  model_knn <- train(make.names(as.factor(signal))~scag_num_1+scag_num_2+scag_num_3+scag_num_4+
                       scag_num_5+scag_num_6+scag_num_6+scag_num_7+scag_num_8+scag_num_9, 
                     data = train_data, method = "knn", trControl = control, 
                   tuneGrid = expand.grid(k = 1:25))
  knn_preds <- predict.train(model_knn, test_data, type = "prob")
  choices$KNN.choice[i] <- list(which(knn_preds[,2] > 0.5))
  
  model_rf <- randomForest(as.factor(signal) ~scag_num_1+scag_num_2+scag_num_3+scag_num_4+
                             scag_num_5+scag_num_6+scag_num_7+scag_num_8+scag_num_9, data = train_data,
                           ntree=100, importance =T)
  rf_preds <- predict(model_rf, newdata = test_data[,2:10], type = "prob")
  choices$RF.choice[i] <- list(which(rf_preds[,2] > 0.5)) 
  
  dscags <- test_data[,2:10]
  means <- colMeans(dscags)
  eu_dists <- NULL
  for (j in 1:20){
    eu_dists <- c(eu_dists, (dist(rbind(dscags[j,],means))))
  }
  eu_max <- sort(eu_dists, decreasing = TRUE)[1:n]
  choices$EU.choice[i] <- list(which(eu_dists %in% eu_max))
  
  mah_dists <- mahalanobis(dscags, means, cov(dscags))
  mah_max <- sort(mah_dists, decreasing = TRUE)[1:n]
  choices$MAH.choice[i] <- list(which(mah_dists %in% mah_max))
}
#NEED TO REWORK DISTANCE MEASURES:
###HOW TO SELECT CUTOFF FOR SIGNAL PREDICTION

false.pos <- data.frame(Euclidean = numeric(K), Mahalanobis = numeric(K), LDA = numeric(K), QDA = numeric(K), Logistic = numeric(K), KNN = numeric(K), Random.Forest = numeric(K))

for(set in 1:K){
  correct <- choices$correct.choice[[set]]
  eu.fp <- mean(choices$EU.choice[[set]] %in% correct)
  false.pos$Euclidean[set] <- ifelse(is.na(eu.fp), 0, 1-eu.fp)
  mah.fp <- mean(choices$MAH.choice[[set]] %in% correct)
  false.pos$Mahalanobis[set] <- ifelse(is.na(mah.fp), 0, 1-mah.fp)
  lda.fp <- mean(choices$LDA.choice[[set]] %in% correct)
  false.pos$LDA[set] <- ifelse(is.na(lda.fp), 0, 1-lda.fp)
  qda.fp <- mean(choices$QDA.choice[[set]] %in% correct)
  false.pos$QDA[set] <- ifelse(is.na(qda.fp), 0, 1-qda.fp)
  log.fp <- mean(choices$LOG.choice[[set]] %in% correct)
  false.pos$Logistic[set] <- ifelse(is.na(log.fp), 0, 1-log.fp)
  knn.fp  <- mean(choices$KNN.choice[[set]] %in% correct)
  false.pos$KNN[set] <- ifelse(is.na(knn.fp), 0, 1-knn.fp)
  rf.fp <- mean(choices$RF.choice[[set]] %in% correct)
  false.pos$Random.Forest[set] <- ifelse(is.na(rf.fp), 0, 1-rf.fp)
}   

table2 <- colMeans(false.pos, na.rm = TRUE)
#AVERAGE RATE OF FALSE POSITIVES
  #EU = .181, MAH - .278, LDA - .026, QDA - .037, LOG - .022, KNN - .018, RF - .030

accuracy <- data.frame(Euclidean = numeric(K), Mahalanobis = numeric(K), LDA = numeric(K), QDA = numeric(K), Logistic = numeric(K), KNN = numeric(K), Random.Forest = numeric(K))

for(set in 1:K){
  correct <- choices$correct.choice[[set]]
  len <- length(correct)
  eu.acc <- sum(choices$EU.choice[[set]] %in% correct)/len
  accuracy$Euclidean[set] <- ifelse(is.na(eu.acc), 0, eu.acc)
  mah.acc <- sum(choices$MAH.choice[[set]] %in% correct)/len
  accuracy$Mahalanobis[set] <- ifelse(is.na(mah.acc), 0, mah.acc)
  lda.acc <- sum(choices$LDA.choice[[set]] %in% correct)/len
  accuracy$LDA[set] <- ifelse(is.na(lda.acc), 0, lda.acc)
  qda.acc <- sum(choices$QDA.choice[[set]] %in% correct)/len
  accuracy$QDA[set] <- ifelse(is.na(qda.acc), 0, qda.acc)
  log.acc <- sum(choices$LOG.choice[[set]] %in% correct)/len
  accuracy$Logistic[set] <- ifelse(is.na(log.acc), 0, log.acc)
  knn.acc  <- sum(choices$KNN.choice[[set]] %in% correct)/len
  accuracy$KNN[set] <- ifelse(is.na(knn.acc), 0, knn.acc)
  rf.acc <- sum(choices$RF.choice[[set]] %in% correct)/len
  accuracy$Random.Forest[set] <- ifelse(is.na(rf.acc), 0, rf.acc)
}   

table3 <- colMeans(accuracy, na.rm = TRUE)
#AVERAGE ACCURACY - % SIGNAL PLOTS IDENTIFIED
#EU = .819, MAH - .722, LDA - .864, QDA - .972, LOG - .962, KNN - .953, RF - .979
```

```{r}
#Accuracy when given number of signal plots
#EU - .826, MAH - .703, LDA - .955, QDA - .985, LOG - .983, KNN - .941, RF - .989
#Accuracy when unknown number of signal plots
#EU = .819, MAH - .722, LDA - .864, QDA - .972, LOG - .962, KNN - .953, RF - .979
```

