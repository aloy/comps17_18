---
title: "Linear_Residuals_Modeling"
author: "Aidan Mullan"
date: "2/21/2018"
output: pdf_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(tidyr)
library(scagnostics)
library(caret)
library(boot)
library(MASS)
library(e1071)
library(randomForest)
library(kernlab)
```

```{r}
data <- read_csv("simulation_data/resid_Scagnostics.csv")
data <- data[,-1]
info <- read_csv("simulation_data/resid_Info.csv")
info <- info[,-1]

data <- data %>%
  spread(key = scag_num, value = scagnostics, sep = "_")

data$signal <- info$signal

```

```{r}
#####
#Logistic Model
#####

#Initial Model
LinRes_glm <- glm(signal ~ scag_num_1+scag_num_2+scag_num_3+scag_num_4+scag_num_5+scag_num_6+scag_num_7+scag_num_8+scag_num_9, data = data, family = "binomial")

summary(LinRes_glm)

LinRes_glm.probs <- predict(LinRes_glm,type="response")
LinRes_glm.predictions <- ifelse(LinRes_glm.probs < .5, 0, 1)
mean(info$signal == LinRes_glm.predictions)
# 97.1% accuracy

1-cv.glm(data, LinRes_glm, K=10)$delta[1]
# 97.5% accuracy


#Updated Model - Removed Stringy
LinRes_glm2 <- glm(signal ~ scag_num_1+scag_num_2+scag_num_3+scag_num_4+scag_num_5+scag_num_6+scag_num_7+scag_num_9, data = data, family = "binomial")
summary(LinRes_glm2)
anova(LinRes_glm, LinRes_glm2, test = "Chisq")

1-cv.glm(data, LinRes_glm2, K=10)$delta[1]
# 97.6% accuracy

LinRes_glm.probs2 <- predict(LinRes_glm2,type="response")
LinRes_glm.predictions2 <- ifelse(LinRes_glm.probs2 < .5, 0, 1)
mean(info$signal == LinRes_glm.predictions2)
# 97.1% accuracy

poor_predict_glm <- info[which(LinRes_glm.predictions != info$signal),]

###215 Errors
```

```{r}
#####
#LDA and QDA Models
#####

LinRes_control <- trainControl(method = "cv", number = 10, classProbs = TRUE)

LinRes_LDA <- train(make.names(as.factor(signal))~.-ID, data = data, method = "lda", trControl = LinRes_control)
LinRes_LDA
# 96.8% accuracy

LinRes_LDA.predict <- predict(LinRes_LDA, type = "raw")
poor_predict_LDA <- info[which(LinRes_LDA.predict != info$signal),]
#This EDA doesn't work, need to fix (change factor names)



LinRes_QDA <- train(make.names(as.factor(signal))~.-ID, data = data, method = "qda", trControl = LinRes_control)
LinRes_QDA
# 96.3% accuracy

LinRes_QDA.predict <- predict(LinRes_QDA, type = "raw")
poor_predict_QDA <- info[which(LinRes_QDA.predict != info$signal),]
#Need to change factor names
```

```{r}
#####
#K-Nearest Neighbors
#####

LinRes_knn <- train(make.names(as.factor(signal))~.-ID, data = data, method = "knn", trControl = LinRes_control, tuneGrid = expand.grid(k = 1:25), metric = "Accuracy")
LinRes_knn
#97.2% accuracy

LinRes_knn.predict <- predict(LinRes_knn, type = "raw")
poor_predict_knn <- info[which(LinRes_knn.predict != info$signal),]
#Need to change factor names



```

```{r}
#####
#Random Forest
#####

#Attempt to use caret package
LinRes_rf <- train(make.names(as.factor(signal))~.-ID, data = data, method = "rf", trControl = LinRes_control, metric = "Accuracy", tuneGrid = expand.grid(mtry = 1:9))
LinRes_rf
# 97.2% accuracy

LinRes_rf.predict <- predict(LinRes_rf, type = "raw")
mean(info$signal == LinRes_rf.predict)
poor_predict_rf <- info[which(LinRes_rf.predict != info$signal),]
#Need to change factor names

```

```{r}
#####
#Support Vector Machines
#####
k <- 10
folds <- cut(sample(seq_len(nrow(data))),  breaks=k, labels=FALSE) 
LinRes_svm.accuracy <- data.frame(accuracy=numeric(k), stringsAsFactors = FALSE)

for (i in seq_len(k)) {
  train_data <- filter(data, folds != i) 
  test_data <- filter(data, folds == i)

  LinRes_svm <- svm(signal ~ scag_num_1+scag_num_2+scag_num_3+scag_num_4+scag_num_5+scag_num_6+scag_num_7+scag_num_8+scag_num_9, data = train_data)

  LinRes_svm.probs <- predict(LinRes_svm, test_data, type = "raw")
  LinRes_svm.predict <- ifelse(LinRes_svm.probs < .5, 0, 1)
  LinRes_svm.accuracy[i,] <- mean(info$signal == LinRes_svm.predict)
}
mean(LinRes_svm.accuracy[,1])
#Accuracy ~59.3%
#Not sure if methodology is sound here, accuracy seems too low
```