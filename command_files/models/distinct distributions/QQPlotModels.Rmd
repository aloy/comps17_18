---
title: "QQPlot Modeling"
author: "Logan Crowl"
date: "2/21/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(MASS)
library(dplyr)
library(tidyverse)
library(boot)
library(ggplot2)
library(randomForest)
library(caret)
library(e1071)
```


```{r}
scags <- read_csv("simulation_data/QQPlots_scagnostics.csv", 
    col_types = cols(X1 = col_skip()))
info <- read_csv("simulation_data/QQPlots_info.csv",
                 col_types = cols(X1 = col_skip()))

scags <- info %>%
  select(ID, distribution, N) %>%
  right_join(scags)

scags <- scags %>%
  mutate(signal = ifelse(distribution == "Normal", "Noise", "Signal")) %>%
  spread(key = scag_num, value = scagnostics, sep = "_")

```

#Logistic Model
```{r}
QQ.glm <- glm(as.factor(signal) ~ scag_num_1+scag_num_2+scag_num_3+scag_num_4+scag_num_5+scag_num_6+scag_num_7+scag_num_8+scag_num_9, data = scags, family = "binomial")

scags <- scags %>% 
  mutate(probs = predict(QQ.glm, type="response"), prediction.glm = ifelse(probs >= 0.5, "Signal", "Noise") )

scags %>% 
  ggplot(aes(x=probs, color= signal)) + 
  geom_density() + ggtitle("Forecasted Signal Probabilities") +
  geom_vline(xintercept = 0.5)

glm.results <- scags %>% summarize(accuracy = mean(signal == prediction.glm), precision = sum(signal == "Signal" &  prediction.glm == "Signal")/sum(prediction.glm == "Signal"), sensitivity = sum(signal == "Signal" & prediction.glm == "Signal")/sum(signal == "Signal"), specificity = sum(signal == "Noise" & prediction.glm == "Noise")/sum(signal == "Noise")) 

#Cross-validate
set.seed(1)
cost <- function(y, pi){mean(abs(y-pi) > 0.5)}
cv.glm1 <- cv.glm(scags, QQ.glm, cost, K = 10)
(accuracy <- 1- cv.glm1$delta[1])


```

It looks like we have cross-validated accuracy of 0.786

#random forest
```{r}
set.seed(1)
QQ.rf <- randomForest(as.factor(signal) ~scag_num_1+scag_num_2+scag_num_3+scag_num_4+scag_num_5+scag_num_6+scag_num_7+scag_num_8+scag_num_9, data = scags, ntree=100, importance =T)

QQ.rf

#choose # of trees
plot(QQ.rf)


```

OOB estimated accuracy of 0.786.  Both models do almost the exact same


#LDA and QDA
```{r}
QQ.control <- trainControl(method = "cv", number = 10, classProbs = TRUE)

set.seed(1)
QQ.lda <- train(as.factor(signal)~ scag_num_1 +scag_num_2+ scag_num_3+ scag_num_4+ scag_num_5+ scag_num_6+ scag_num_7+ scag_num_8+scag_num_9 , data = scags, method = "lda", trControl = QQ.control)

scags$prediction.lda <- predict(QQ.lda, type = "raw")

(lda.results <- scags %>% summarize(accuracy = mean(signal == prediction.lda), precision = sum(signal == "Signal" &  prediction.lda == "Signal")/sum(prediction.lda == "Signal"), sensitivity = sum(signal == "Signal" & prediction.lda == "Signal")/sum(signal == "Signal"), specificity = sum(signal == "Noise" & prediction.lda == "Noise")/sum(signal == "Noise"))) 

#Note that we have 85% specificity, so we do a pretty good job of correctly identifying normal plots
#82.6% of our signal predictions were correct.
#We only pick out 69% of the total signal plots

QQ.qda <- train(as.factor(signal)~ scag_num_1 +scag_num_2+ scag_num_3+ scag_num_4+ scag_num_5+ scag_num_6+ scag_num_7+ scag_num_8+scag_num_9 , data = scags, method = "qda", trControl = QQ.control)

scags$prediction.qda <- predict(QQ.qda, type = "raw")

(qda.results <- scags %>% summarize(accuracy = mean(signal == prediction.qda), precision = sum(signal == "Signal" &  prediction.qda == "Signal")/sum(prediction.qda == "Signal"), sensitivity = sum(signal == "Signal" & prediction.qda == "Signal")/sum(signal == "Signal"), specificity = sum(signal == "Noise" & prediction.qda == "Noise")/sum(signal == "Noise")))

#QDA has worse sensitivity (63.3% vs. 68.8%) and accuracy (75.6% vs. 77.2%)
#Better precision and specificity
```

#K-nn
```{r}
QQ.control <- trainControl(method = "cv", number = 10, classProbs = TRUE)

set.seed(1)
QQ.knn <- train(as.factor(signal)~ scag_num_1 +scag_num_2+ scag_num_3+ scag_num_4+ scag_num_5+ scag_num_6+ scag_num_7+ scag_num_8+scag_num_9 , data = scags, method = "knn", trControl = QQ.control, tuneGrid = expand.grid(k = 1:25), metric = "Accuracy")

QQ.knn

scags$prediction.knn <- predict(QQ.knn, type = "raw")

knn.results <- scags %>% summarize(accuracy = mean(signal == prediction.knn), precision = sum(signal == "Signal" &  prediction.knn == "Signal")/sum(prediction.knn == "Signal"), sensitivity = sum(signal == "Signal" & prediction.knn == "Signal")/sum(signal == "Signal"), specificity = sum(signal == "Noise" & prediction.knn == "Noise")/sum(signal == "Noise")) 

#Our precision is really really good (93%)
#accuracy is slightly better than QDA, but similar to other models

```


##WE NEED MORE THAN JUST SCAGNOSTICS##

#logistic2
```{r}
QQ.glm2 <- glm(as.factor(signal) ~ scag_num_1+scag_num_2+scag_num_3+scag_num_4+scag_num_5+scag_num_6+scag_num_7+scag_num_8+scag_num_9 + AD_pvalue + SW_pvalue + CVM_pvalue + lillie_pvalue + pearson_pvalue + SF_pvalue, data = scags, family = "binomial")

summary(QQ.glm2)

#Cross-validate
set.seed(1)
cost <- function(y, pi){mean(abs(y-pi) > 0.5)}
cv.glm2 <- cv.glm(scags, QQ.glm2, cost, K = 10)
(accuracy <- 1- cv.glm2$delta[1])

#let's only use a couple
QQ.glm3 <- glm(as.factor(signal) ~ scag_num_1+scag_num_2+scag_num_3+scag_num_4+scag_num_5+scag_num_6+scag_num_7+scag_num_8+scag_num_9 + AD_pvalue + SW_pvalue + CVM_pvalue + lillie_pvalue + pearson_pvalue + SF_pvalue, data = scags, family = "binomial")
#Cross-validate
set.seed(1)
cost <- function(y, pi){mean(abs(y-pi) > 0.5)}
cv.glm2 <- cv.glm(scags, QQ.glm2, cost, K = 10)
(accuracy <- 1- cv.glm2$delta[1])
```

Our cross validated accuracy increases slightly (78.6% to 81.8%)

#random forest
```{r}
set.seed(1)
QQ.rf <- randomForest(as.factor(signal) ~scag_num_1+scag_num_2+scag_num_3+scag_num_4+scag_num_5+scag_num_6+scag_num_7+scag_num_8+scag_num_9, data = scags, ntree=100, importance =T)

QQ.rf

#choose # of trees
plot(QQ.rf)


```

OOB estimated accuracy of 0.786.  Both models do almost the exact same


#LDA and QDA
```{r}
QQ.control <- trainControl(method = "cv", number = 10, classProbs = TRUE)

set.seed(1)
QQ.lda <- train(as.factor(signal)~ scag_num_1 +scag_num_2+ scag_num_3+ scag_num_4+ scag_num_5+ scag_num_6+ scag_num_7+ scag_num_8+scag_num_9 , data = scags, method = "lda", trControl = QQ.control)

scags$prediction.lda <- predict(QQ.lda, type = "raw")

(lda.results <- scags %>% summarize(accuracy = mean(signal == prediction.lda), precision = sum(signal == "Signal" &  prediction.lda == "Signal")/sum(prediction.lda == "Signal"), sensitivity = sum(signal == "Signal" & prediction.lda == "Signal")/sum(signal == "Signal"), specificity = sum(signal == "Noise" & prediction.lda == "Noise")/sum(signal == "Noise"))) 

#Note that we have 85% specificity, so we do a pretty good job of correctly identifying normal plots
#82.6% of our signal predictions were correct.
#We only pick out 69% of the total signal plots

QQ.qda <- train(as.factor(signal)~ scag_num_1 +scag_num_2+ scag_num_3+ scag_num_4+ scag_num_5+ scag_num_6+ scag_num_7+ scag_num_8+scag_num_9 , data = scags, method = "qda", trControl = QQ.control)

scags$prediction.qda <- predict(QQ.qda, type = "raw")

(qda.results <- scags %>% summarize(accuracy = mean(signal == prediction.qda), precision = sum(signal == "Signal" &  prediction.qda == "Signal")/sum(prediction.qda == "Signal"), sensitivity = sum(signal == "Signal" & prediction.qda == "Signal")/sum(signal == "Signal"), specificity = sum(signal == "Noise" & prediction.qda == "Noise")/sum(signal == "Noise")))

#QDA has worse sensitivity (63.3% vs. 68.8%) and accuracy (75.6% vs. 77.2%)
#Better precision and specificity
```

#K-nn
```{r}
QQ.control <- trainControl(method = "cv", number = 10, classProbs = TRUE)

set.seed(1)
QQ.knn <- train(as.factor(signal)~ scag_num_1 +scag_num_2+ scag_num_3+ scag_num_4+ scag_num_5+ scag_num_6+ scag_num_7+ scag_num_8+scag_num_9 , data = scags, method = "knn", trControl = QQ.control, tuneGrid = expand.grid(k = 1:25), metric = "Accuracy")

QQ.knn

scags$prediction.knn <- predict(QQ.knn, type = "raw")

knn.results <- scags %>% summarize(accuracy = mean(signal == prediction.knn), precision = sum(signal == "Signal" &  prediction.knn == "Signal")/sum(prediction.knn == "Signal"), sensitivity = sum(signal == "Signal" & prediction.knn == "Signal")/sum(signal == "Signal"), specificity = sum(signal == "Noise" & prediction.knn == "Noise")/sum(signal == "Noise")) 

#Our precision is really really good (93%)
#accuracy is slightly better than QDA, but similar to other models

```
