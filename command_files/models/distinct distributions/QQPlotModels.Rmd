---
title: "QQPlot Modeling"
author: "Logan Crowl"
date: "2/21/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(MASS)
library(dplyr)
library(tidyverse)
library(boot)
library(ggplot2)
library(randomForest)
library(caret)
library(e1071)
```


```{r}
scags <- read_csv("simulation_data/QQPlots_scagnostics.csv", 
    col_types = cols(X1 = col_skip()))
info <- read_csv("simulation_data/QQPlots_info.csv",
                 col_types = cols(X1 = col_skip()))

scags <- info %>%
  dplyr::select(ID, distribution, N) %>%
  right_join(scags)

scags <- scags %>%
  mutate(signal = ifelse(distribution == "Normal", "Noise", "Signal")) %>%
  spread(key = scag_num, value = scagnostics, sep = "_")

```

#Logistic Model
```{r}
QQ.glm <- glm(as.factor(signal) ~ scag_num_1+scag_num_2+scag_num_3+scag_num_4+scag_num_5+scag_num_6+scag_num_7+scag_num_8+scag_num_9, data = scags, family = "binomial")

scags <- scags %>% 
  mutate(probs = predict(QQ.glm, type="response"), prediction.glm = ifelse(probs >= 0.5, "Signal", "Noise") )

scags %>% 
  ggplot(aes(x=probs, color= signal)) + 
  geom_density() + ggtitle("Forecasted Signal Probabilities") +
  geom_vline(xintercept = 0.5)

glm.results <- scags %>% summarize(accuracy = mean(signal == prediction.glm), precision = sum(signal == "Signal" &  prediction.glm == "Signal")/sum(prediction.glm == "Signal"), sensitivity = sum(signal == "Signal" & prediction.glm == "Signal")/sum(signal == "Signal"), specificity = sum(signal == "Noise" & prediction.glm == "Noise")/sum(signal == "Noise")) 

#Cross-validate
set.seed(1)
cost <- function(y, pi){mean(abs(y-pi) > 0.5)}
cv.glm1 <- cv.glm(scags, QQ.glm, cost, K = 10)
(accuracy1 <- 1- cv.glm1$delta[1])


```

It looks like we have cross-validated accuracy of 0.786.

#random forest
```{r}
set.seed(1)
QQ.rf <- randomForest(as.factor(signal) ~scag_num_1+scag_num_2+scag_num_3+scag_num_4+scag_num_5+scag_num_6+scag_num_7+scag_num_8+scag_num_9, data = scags, ntree=100, importance =T)

QQ.rf

#choose # of trees
plot(QQ.rf)

#100 is fine
```

OOB estimated accuracy of 0.786.  Both models do almost the exact same


#LDA and QDA
```{r}
QQ.control <- trainControl(method = "cv", number = 10, classProbs = TRUE)

set.seed(1)
QQ.lda <- train(as.factor(signal)~ scag_num_1 +scag_num_2+ scag_num_3+ scag_num_4+ scag_num_5+ scag_num_6+ scag_num_7+ scag_num_8+scag_num_9 , data = scags, method = "lda", trControl = QQ.control)

scags$prediction.lda <- predict(QQ.lda, type = "raw")

(lda.results <- scags %>% summarize(accuracy = mean(signal == prediction.lda), precision = sum(signal == "Signal" &  prediction.lda == "Signal")/sum(prediction.lda == "Signal"), sensitivity = sum(signal == "Signal" & prediction.lda == "Signal")/sum(signal == "Signal"), specificity = sum(signal == "Noise" & prediction.lda == "Noise")/sum(signal == "Noise"))) 

#Note that we have 85% specificity, so we do a pretty good job of correctly identifying normal plots
#82.6% of our signal predictions were correct.
#We only pick out 69% of the total signal plots

QQ.qda <- train(as.factor(signal)~ scag_num_1 +scag_num_2+ scag_num_3+ scag_num_4+ scag_num_5+ scag_num_6+ scag_num_7+ scag_num_8+scag_num_9 , data = scags, method = "qda", trControl = QQ.control)

scags$prediction.qda <- predict(QQ.qda, type = "raw")

(qda.results <- scags %>% summarize(accuracy = mean(signal == prediction.qda), precision = sum(signal == "Signal" &  prediction.qda == "Signal")/sum(prediction.qda == "Signal"), sensitivity = sum(signal == "Signal" & prediction.qda == "Signal")/sum(signal == "Signal"), specificity = sum(signal == "Noise" & prediction.qda == "Noise")/sum(signal == "Noise")))

#QDA has worse sensitivity (63.3% vs. 68.8%) and accuracy (75.6% vs. 77.2%)
#Better precision and specificity
```

#K-nn
```{r}
QQ.control <- trainControl(method = "cv", number = 10, classProbs = TRUE)

set.seed(1)
QQ.knn <- train(as.factor(signal)~ scag_num_1 +scag_num_2+ scag_num_3+ scag_num_4+ scag_num_5+ scag_num_6+ scag_num_7+ scag_num_8+scag_num_9 , data = scags, method = "knn", trControl = QQ.control, tuneGrid = expand.grid(k = 1:25), metric = "Accuracy")

QQ.knn

scags$prediction.knn <- predict(QQ.knn, type = "raw")

knn.results <- scags %>% summarize(accuracy = mean(signal == prediction.knn), precision = sum(signal == "Signal" &  prediction.knn == "Signal")/sum(prediction.knn == "Signal"), sensitivity = sum(signal == "Signal" & prediction.knn == "Signal")/sum(signal == "Signal"), specificity = sum(signal == "Noise" & prediction.knn == "Noise")/sum(signal == "Noise")) 

#Our precision is really really good (93%)
#accuracy is slightly better than QDA, but similar to other models

```


##WE NEED MORE THAN JUST SCAGNOSTICS##

#logistic2
```{r}
QQ.glm2 <- glm(as.factor(signal) ~ scag_num_1+scag_num_2+scag_num_3+scag_num_4+scag_num_5+scag_num_6+scag_num_7+scag_num_8+scag_num_9 + AD_pvalue + SW_pvalue + CVM_pvalue + lillie_pvalue + pearson_pvalue + SF_pvalue, data = scags, family = "binomial")

summary(QQ.glm2)

#Cross-validate
set.seed(1)
cost <- function(y, pi){mean(abs(y-pi) > 0.5)}
cv.glm2 <- cv.glm(scags, QQ.glm2, cost, K = 10)
(accuracy2 <- 1- cv.glm2$delta[1])

```

Our cross validated accuracy increases slightly (78.6% to 81.8%)

#random forest
```{r}
set.seed(1)
QQ.rf2 <- randomForest(as.factor(signal) ~scag_num_1+scag_num_2+scag_num_3+scag_num_4+scag_num_5+scag_num_6+scag_num_7+scag_num_8+scag_num_9 + AD_pvalue + SW_pvalue + CVM_pvalue + lillie_pvalue + pearson_pvalue + SF_pvalue, data = scags, ntree=100, importance =T)

QQ.rf2

```

OOB estimated accuracy of 0.8403.  Random Forest does better.


#LDA and QDA
```{r}
QQ.control <- trainControl(method = "cv", number = 10, classProbs = TRUE)

set.seed(1)
QQ.lda2 <- train(as.factor(signal)~ scag_num_1 +scag_num_2+ scag_num_3+ scag_num_4+ scag_num_5+ scag_num_6+ scag_num_7+ scag_num_8+scag_num_9 + AD_pvalue + SW_pvalue + CVM_pvalue + lillie_pvalue + pearson_pvalue + SF_pvalue, data = scags, method = "lda", trControl = QQ.control)

scags$prediction.lda2 <- predict(QQ.lda2, type = "raw")

(lda.results2 <- scags %>% summarize(accuracy = mean(signal == prediction.lda2), precision = sum(signal == "Signal" &  prediction.lda2 == "Signal")/sum(prediction.lda2 == "Signal"), sensitivity = sum(signal == "Signal" & prediction.lda2 == "Signal")/sum(signal == "Signal"), specificity = sum(signal == "Noise" & prediction.lda2 == "Noise")/sum(signal == "Noise"))) 

#Note that we have 85% specificity, so we do a pretty good job of correctly identifying normal plots
#82.6% of our signal predictions were correct.
#We only pick out 69% of the total signal plots

QQ.qda2 <- train(as.factor(signal)~ scag_num_1 +scag_num_2+ scag_num_3+ scag_num_4+ scag_num_5+ scag_num_6+ scag_num_7+ scag_num_8+scag_num_9 + AD_pvalue + SW_pvalue + CVM_pvalue + lillie_pvalue + pearson_pvalue + SF_pvalue, data = scags, method = "qda", trControl = QQ.control)

scags$prediction.qda2 <- predict(QQ.qda2, type = "raw")

(qda.results2 <- scags %>% summarize(accuracy = mean(signal == prediction.qda2), precision = sum(signal == "Signal" &  prediction.qda2 == "Signal")/sum(prediction.qda2 == "Signal"), sensitivity = sum(signal == "Signal" & prediction.qda2 == "Signal")/sum(signal == "Signal"), specificity = sum(signal == "Noise" & prediction.qda2 == "Noise")/sum(signal == "Noise")))

#Everything gets better
```

~81% accuracy

Both QDA and LDA get better when normality test results are added.  Big jumps in sensitivity. QDA now does better by most measures (everything but sensitivity).

#K-nn
```{r}
QQ.control <- trainControl(method = "cv", number = 5, classProbs = TRUE)

set.seed(1)
QQ.knn2 <- train(as.factor(signal)~ scag_num_1 +scag_num_2+ scag_num_3+ scag_num_4+ scag_num_5+ scag_num_6+ scag_num_7+ scag_num_8+scag_num_9 + AD_pvalue + SW_pvalue + CVM_pvalue + lillie_pvalue + pearson_pvalue + SF_pvalue, data = scags, method = "knn", trControl = QQ.control, tuneGrid = expand.grid(k = 1:25), metric = "Accuracy")

QQ.knn2
```

Cross validated accuracy of 83.5%.



##Let's test out our home-made scagnostics

#EDA for new scags
```{r}
ggplot(data = scags) + geom_histogram(aes(x = deviation.base)) + facet_wrap(~ distribution)

ggplot(data = scags) + geom_histogram(aes(x = deviation.add)) + facet_wrap(~ distribution)

ggplot(data = scags) + geom_histogram(aes(x = deviation.addLog)) + facet_wrap(~ distribution)

ggplot(data = scags) + geom_histogram(aes(x = deviation.addSq)) + facet_wrap(~ distribution)

ggplot(data = scags) + geom_histogram(aes(x = deviation.mult)) + facet_wrap(~ distribution)

ggplot(data = scags) + geom_histogram(aes(x = deviation.multLog)) + facet_wrap(~ distribution)

ggplot(data = scags) + geom_histogram(aes(x = deviation.multSq)) + facet_wrap(~ distribution)

#Explore T vs. Normal
ggplot(data = scags %>% filter(distribution %in% c("Normal", "T"))) + geom_boxplot(aes(y = deviation.base, x = distribution))

ggplot(data = scags %>% filter(distribution %in% c("Normal", "T"))) + geom_boxplot(aes(y = deviation.add, x = distribution))

ggplot(data = scags %>% filter(distribution %in% c("Normal", "T"))) + geom_boxplot(aes(y = deviation.addLog, x = distribution))

ggplot(data = scags %>% filter(distribution %in% c("Normal", "T"))) + geom_boxplot(aes(y = deviation.addSq, x = distribution))

ggplot(data = scags %>% filter(distribution %in% c("Normal", "T"))) + geom_boxplot(aes(y = deviation.mult, x = distribution))

ggplot(data = scags %>% filter(distribution %in% c("Normal", "T"))) + geom_boxplot(aes(y = deviation.multLog, x = distribution))

ggplot(data = scags %>% filter(distribution %in% c("Normal", "T"))) + geom_boxplot(aes(y = deviation.multSq, x = distribution))
```

#logistic3
```{r}
#deviation.base
QQ.glm3 <- glm(as.factor(signal) ~ scag_num_1+scag_num_2+scag_num_3+scag_num_4+scag_num_5+scag_num_6+scag_num_7+scag_num_8+scag_num_9 + deviation.base, data = scags, family = "binomial")

summary(QQ.glm3)

#Cross-validate
set.seed(1)
cost <- function(y, pi){mean(abs(y-pi) > 0.5)}
cv.glm3 <- cv.glm(scags, QQ.glm3, cost, K = 10)
(accuracy3 <- 1- cv.glm3$delta[1])

#deviation.add
QQ.glm4 <- glm(as.factor(signal) ~ scag_num_1+scag_num_2+scag_num_3+scag_num_4+scag_num_5+scag_num_6+scag_num_7+scag_num_8+scag_num_9 + deviation.add, data = scags, family = "binomial")

summary(QQ.glm4)

#Cross-validate
set.seed(1)
cv.glm4<- cv.glm(scags, QQ.glm4, cost, K = 10)
(accuracy4 <- 1- cv.glm4$delta[1])

#deviation.addLog
QQ.glm5 <- glm(as.factor(signal) ~ scag_num_1+scag_num_2+scag_num_3+scag_num_4+scag_num_5+scag_num_6+scag_num_7+scag_num_8+scag_num_9 + deviation.addLog, data = scags, family = "binomial")

summary(QQ.glm5)

#Cross-validate
set.seed(1)
cv.glm5<- cv.glm(scags, QQ.glm5, cost, K = 10)
(accuracy5 <- 1- cv.glm5$delta[1])

#deviation.addSq
QQ.glm6 <- glm(as.factor(signal) ~ scag_num_1+scag_num_2+scag_num_3+scag_num_4+scag_num_5+scag_num_6+scag_num_7+scag_num_8+scag_num_9 + deviation.addSq, data = scags, family = "binomial")

summary(QQ.glm6)

#Cross-validate
set.seed(1)
cv.glm6<- cv.glm(scags, QQ.glm6, cost, K = 10)
(accuracy6 <- 1- cv.glm6$delta[1])

#deviation.mult
QQ.glm7 <- glm(as.factor(signal) ~ scag_num_1+scag_num_2+scag_num_3+scag_num_4+scag_num_5+scag_num_6+scag_num_7+scag_num_8+scag_num_9 + deviation.mult, data = scags, family = "binomial")

summary(QQ.glm7)

#Cross-validate
set.seed(1)
cv.glm7<- cv.glm(scags, QQ.glm7, cost, K = 10)
(accuracy7 <- 1- cv.glm7$delta[1])


#deviation.multLog
QQ.glm8 <- glm(as.factor(signal) ~ scag_num_1+scag_num_2+scag_num_3+scag_num_4+scag_num_5+scag_num_6+scag_num_7+scag_num_8+scag_num_9 + deviation.multLog, data = scags, family = "binomial")

summary(QQ.glm8)

#Cross-validate
set.seed(1)
cv.glm8<- cv.glm(scags, QQ.glm8, cost, K = 10)
(accuracy8 <- 1- cv.glm8$delta[1])


#deviation.multSq
QQ.glm9 <- glm(as.factor(signal) ~ scag_num_1+scag_num_2+scag_num_3+scag_num_4+scag_num_5+scag_num_6+scag_num_7+scag_num_8+scag_num_9 + deviation.multSq, data = scags, family = "binomial")

summary(QQ.glm9)

#Cross-validate
set.seed(1)
cv.glm9<- cv.glm(scags, QQ.glm9, cost, K = 10)
(accuracy9 <- 1- cv.glm9$delta[1])
```


#random forest
```{r}
#deviation.base
set.seed(1)
QQ.rf3 <- randomForest(as.factor(signal) ~scag_num_1+scag_num_2+scag_num_3+scag_num_4+scag_num_5+scag_num_6+scag_num_7+scag_num_8+scag_num_9 + deviation.base, data = scags, ntree=100, importance =T)

QQ.rf3 #error rate = 16.34%

varImpPlot(QQ.rf3)


#deviation.add
set.seed(1)
QQ.rf4 <- randomForest(as.factor(signal) ~scag_num_1+scag_num_2+scag_num_3+scag_num_4+scag_num_5+scag_num_6+scag_num_7+scag_num_8+scag_num_9 + deviation.add, data = scags, ntree=100, importance =T)

QQ.rf4 #error rate = 16.55%

#deviation.addLog
set.seed(1)
QQ.rf5 <- randomForest(as.factor(signal) ~scag_num_1+scag_num_2+scag_num_3+scag_num_4+scag_num_5+scag_num_6+scag_num_7+scag_num_8+scag_num_9 + deviation.addLog, data = scags, ntree=100, importance =T)

QQ.rf5 #error rate = 16.51%

#deviation.addSq
set.seed(1)
QQ.rf6 <- randomForest(as.factor(signal) ~scag_num_1+scag_num_2+scag_num_3+scag_num_4+scag_num_5+scag_num_6+scag_num_7+scag_num_8+scag_num_9 + deviation.addSq, data = scags, ntree=100, importance =T)

QQ.rf6 #error rate = 16.19%

#deviation.mult
set.seed(1)
QQ.rf7 <- randomForest(as.factor(signal) ~scag_num_1+scag_num_2+scag_num_3+scag_num_4+scag_num_5+scag_num_6+scag_num_7+scag_num_8+scag_num_9 + deviation.mult, data = scags, ntree=100, importance =T)

QQ.rf7 #error rate = 16.28%

#deviation.multLog
set.seed(1)
QQ.rf8 <- randomForest(as.factor(signal) ~scag_num_1+scag_num_2+scag_num_3+scag_num_4+scag_num_5+scag_num_6+scag_num_7+scag_num_8+scag_num_9 + deviation.multLog, data = scags, ntree=100, importance =T)

QQ.rf8 #error rate = 16.18%
 
#deviation.multSq
set.seed(1)
QQ.rf9 <- randomForest(as.factor(signal) ~scag_num_1+scag_num_2+scag_num_3+scag_num_4+scag_num_5+scag_num_6+scag_num_7+scag_num_8+scag_num_9 + deviation.multSq, data = scags, ntree=100, importance =T)

QQ.rf9 #error rate = 15.99%
```

OOB estimated accuracy of 0.8401.  Random Forest now does a little worse.


#LDA and QDA
```{r}
QQ.control <- trainControl(method = "cv", number = 5, classProbs = TRUE)

set.seed(1)
QQ.lda3 <- train(as.factor(signal)~ scag_num_1 +scag_num_2+ scag_num_3+ scag_num_4+ scag_num_5+ scag_num_6+ scag_num_7+ scag_num_8+scag_num_9 + deviation.multSq, data = scags, method = "lda", trControl = QQ.control)

scags$prediction.lda3 <- predict(QQ.lda3, type = "raw")

(lda.results3 <- scags %>% summarize(accuracy = mean(signal == prediction.lda3), precision = sum(signal == "Signal" &  prediction.lda3 == "Signal")/sum(prediction.lda3 == "Signal"), sensitivity = sum(signal == "Signal" & prediction.lda3 == "Signal")/sum(signal == "Signal"), specificity = sum(signal == "Noise" & prediction.lda3 == "Noise")/sum(signal == "Noise"))) 

lda.results2
lda.results

#Our precision and specificity are great, but our accuracy and sensitivity are worse than when Norm test p-values are included. Everything has improved from just regular scagnostics model. 

QQ.qda3 <- train(as.factor(signal)~ scag_num_1 +scag_num_2+ scag_num_3+ scag_num_4+ scag_num_5+ scag_num_6+ scag_num_7+ scag_num_8+scag_num_9 + deviation.multSq, data = scags, method = "qda", trControl = QQ.control)

scags$prediction.qda3 <- predict(QQ.qda3, type = "raw")

(qda.results3 <- scags %>% summarize(accuracy = mean(signal == prediction.qda3), precision = sum(signal == "Signal" &  prediction.qda3 == "Signal")/sum(prediction.qda3 == "Signal"), sensitivity = sum(signal == "Signal" & prediction.qda3 == "Signal")/sum(signal == "Signal"), specificity = sum(signal == "Noise" & prediction.qda3 == "Noise")/sum(signal == "Noise")))

qda.results2
qda.results

#For LDA and QDA, specificity and precision is better than norm tests, but sensitivity is down
```

~81% accuracy

#K-nn
```{r}
QQ.control <- trainControl(method = "cv", number = 5, classProbs = TRUE)

set.seed(1)
QQ.knn3 <- train(as.factor(signal)~ scag_num_1 +scag_num_2+ scag_num_3+ scag_num_4+ scag_num_5+ scag_num_6+ scag_num_7+ scag_num_8+scag_num_9 + deviation.base, data = scags, method = "knn", trControl = QQ.control, tuneGrid = expand.grid(k = 1:25), metric = "Accuracy")

QQ.knn3
QQ.knn2

QQ.knn4 <- train(as.factor(signal)~ scag_num_1 +scag_num_2+ scag_num_3+ scag_num_4+ scag_num_5+ scag_num_6+ scag_num_7+ scag_num_8+scag_num_9 + scale(deviation.multSq), data = scags, method = "knn", trControl = QQ.control, tuneGrid = expand.grid(k = 15:25), metric = "Accuracy")

QQ.knn4
QQ.knn2
```

KNN does better than norm test knn! (83.5% vs. 84.2%). 

#Does our new scagnostic do better or worse that anderson darling
```{r}
scags$adpredict <- ifelse(scags$AD_pvalue < 0.05, "Signal", "Noise")
mean(scags$adpredict == scags$signal)
```

